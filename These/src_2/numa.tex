\section{How NUMA effect can affect efficiency ?}
Performance of a computer mostly depends on two things : processors and memories.
%
In this chapter, I will focus on the memory problem.
%
When a program wants to read memory, it is subjected to a memory penalty.
%
This penalty is the sum of two constraints :
\begin{itemize}
        \item latency : the time between the ask of a memory of the first byte received;
        \item bandwith : the maximum of byte per second that can be send/receive
\end{itemize}

The first approach to connect multiple memories with multiple processors is to use a memory controller (see section~\ref{sec:smp}).
%
Processors and memories are connected to the controller by a bus.
%
A bus conflict can happen if two processors want to access to the same memory.
%
The controller must avoid bus conflicts by ordering the memory accesses.
%
It also must keep a low latency access to the memory by all processors.
%
But the low latency constrain isn't possible to satisfy when the number of processors is too high.
%
One solution could be to distribute memory over all processors.


We call this solution NUMA, which is the acronym of Non Uniform Memory Access.
%
In this architecture, each processor has a memory bank and a connection with one or more processors (see Fig.~\ref{fig:numa}).
%
That means that a processor doesn't have the same memory penalty depending on the physical memory address it accesses.
%
This is the NUMA effect.




%-------------------------------
\subsection{Next-touch}
The next-touch policy
%


%-------------------------------
\subsection{Automatic solution}
Most frameworks for multi-core platforms don't handle memory locality, some try to improve data locality between tasks by leaving the user with the choice of the next task to schedule (e.g., {\em continuation} in TBB).
%
In the case of a task scheduler for heterogeneous platforms, data need to be moved to the target platform.
Therefore, in this case, the scheduler must know which data are used by each task to move them at the right time.
%
Unfortunately none of these schedulers care about NUMA in their scheduling algorithms to reduce the overhead of data movement.


The Linux kernel 3.13 add an automatic way to optimize NUMA placement over the execution of the program.
%
The idea is simple, regularly virtual pages are unmapped and the next access to these pages determine a better placement.
%
A scan rate value can be set with the sysctl interface.
%
But this implementation has two major problems.
%
First, it can only be enable or disable for the entire system not for a specific program or, better, for a memory range.
%
Second, the best scan rate value for a program is almost impossible to find.
%
It is a trade-off between the overhead of the page faults and the gain obtain by moving data closer.
%
We will compare this solution with the homemade userland solution describe in the next section.
