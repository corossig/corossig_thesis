\section{Definition of granularity}
In computer programming, we use tasks model to express the potential application parallelism in an abstract way.
%
This model is independent from the available hardware resources and was once popularized by tools such as Cilk~\cite{cilk} appeared in 1994.
%
The widespread availability of multi-core processors recently revived the popularity of task scheduling frameworks~\cite{taskscomparison} as exemplified by tools such as Intel TBB~\cite{Intel::TBB} and OpenMP~3.0's task support~\cite{openmptasks} for regular multi-core platforms, or StarSs/OmpSs derivatives~\cite{ompss} as well as StarPU~\cite{starpu} and X-Kaapi~\cite{xkaapi} for heterogeneous platforms.
%
In these model, granularity refers to the number of CPU instructions inside a task.
%
The tasks grain size depends on the algorithm to parallelize and also of the runtime task scheduler.
%
Finding the perfect grain size is painful.
%
If the grain size is too big, the runtime cannot fairly balance the computation over all cores of the processor.
%
But on the contrary, if the grain size is too small, the runtime overhead kill performances.


Several related works have been conducted in the past to address the issue of adapting the task grain size to the amount of available computing units.
%
Many related works partially address this grain size issue by promoting cache-oblivious techniques for a specific class of
applications such as recursive, divide-and-conquer codes or recursively partitioned loops~\cite{unifieddataflow,Intel::TBB,cilk,xkaapi,taskscomparison}.
%
Works such as the SCOOPP framework~\cite{scoopp} provides means for the applications to control the task grain size.
%
However, the grain size selection issue is still up to the application programmer.


On the theoretical side, general task scheduling has been heavily studied for a long time now~\cite{Khan94acomparison,heft}.
%
Works on task grain adaptiveness have been scarcer, but do exist.