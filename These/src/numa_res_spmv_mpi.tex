\subsubsection{Mémoire distribuée}
Pour paralléliser un produit matrice vecteur creux en mémoire distribuée, il faut tout d'abord distribuer la matrice sur tous les processus.
%
Chaque processus devra s'occuper d'un ensemble de lignes de la matrice.
%
Cette répartition sera statique et les indices des lignes seront les mêmes pour la distribution des vecteurs (Fig.~\ref{fig:spmv_mpi}).
%
Nous pouvons voir que certaines éléments du vecteur $x$ doivent être partagées.
%
Pour cela, l'algorithme démarrera par une phase de communication de ces éléments.
%
Puis chaque processus pourra effectuer la multiplication de ces lignes de matrice et stocker le résultat dans un vecteur $y$ local.

%   (-_-)   %
\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{spmv_mpi}
  \caption{Distribution des données utilisées par le produit matrice vecteur creux.}
  \label{fig:spmv_mpi}
\end{figure}

Hormis la phase de communication au début de l'algorithme, le SpMV se parallélise très bien en mémoire distribuée.
%
Cette phase de communication peut être, dans certains cas, recouverte par du calcul.
%
Il suffit de multiplier en premier les lignes de la matrice qui ne dépendent pas des éléments distants.
%
Puis une fois les éléments reçus, il est possible de multiplier les lignes restantes.



La performance atteinte en mémoire distribuée correspondent à la performance crête du SpMV.
%
En effet, de par sa nature distribuée, les pénalités mémoires NUMA sont minimales.
%
Chaque processus allouera la mémoire dont il a besoin sur son propre banc NUMA et si celui-ci n'est pas déplacé, tous ces accès mémoires seront optimaux.
%
Nous pouvons donc estimer cette performance sera la borne maximale à atteindre lorsque l'on travaillera en mémoire partagée.




Le roofline model prédit un algorithme limité en performance par la bande passante mémoire.
%
Or, cette bande passante mémoire est partagée entre les coeurs d'un même banc NUMA.
%
L'accélération obtenue sera donc limité par la bande passante mémoire.
%
Sur la machine Rostand, la bande passante mémoire limite grandement cette accélération (Fig.~\ref{fig:res_spmv_mpi_rostand}).
%
Avec un cas à 8 variables primaires, nous obtenons une accélération maximale de 3,8.
%
La capacité de calcul mesuré avec 12 coeurs est de 4,96~GFlops, cela correspond à la prédiction du roofline model.


%   (-_-)   %
\begin{figure}
  \centering
  \includegraphics[width=0.7\textwidth]{res_spmv_mpi}
  \caption{Accélération du produit matrice vecteur creux sur Rostand en mémoire distribuée.}
  \label{fig:res_spmv_mpi_rostand}
\end{figure}



Sur Manumanu, nous avons beaucoup plus de banc NUMA, ce qui signifie que nous aurons plus de bande passante mémoire à notre disposition.
%
Nous pouvons donc espérer avoir de meilleurs résultats que sur Rostand.
%
Il faut aussi prendre en compte une bande passante mémoire plus élevée sur les bancs NUMA de Manumanu que sur ceux de Rostand.
%
Les processus MPI sont alloués en mode compact, c'est à dire qu'ils sont distribués de façon à utiliser un minimum de noeuds NUMA.
%
Sur 1 banc NUMA, nous avons une accélération de 5 avec 8 variables primaires (Fig.~\ref{fig:res_spmv_mpi_manumanu}).
%
Cette accélération monte à 110 avec l'utilisation des 20 bancs NUMA et des 160 coeurs.


%   (-_-)   %
\begin{figure}
  \centering
  \includegraphics[width=0.7\textwidth]{res_spmv_mpi_manu}
  \caption{Accélération du produit matrice vecteur creux sur Manumanu en mémoire distribuée.}
  \label{fig:res_spmv_mpi_manumanu}
\end{figure}
