%+++++++++++++++++++++++++++++++
\section{Multi-core parallelism}
%-------------------------------
\subsection{Parallel for loop}
  \begin{itemize}
    \item OpenMP
  \end{itemize}
%-------------------------------
\subsection{Task paradigm}
  \begin{itemize}
    \item PaRSEC
    \item
  \end{itemize}
%+++++++++++++++++++++++++++++++


\subsection{Runtime}
A runtime is a piece of software used by others software to abstract part of the system.
%
There are several type of runtime.
%
Some high level programming languages use runtime, for example Java has a runtime for garbage collection.
%
Also some implementations of MPI have a runtime.
%
Task based programming also tend to have a runtime.
%
In the following part, we will focus on runtime support for task based programming.



The main interest of runtime in task based programming is to schedule tasks while respecting strict dependencies order.
%Le rôle des supports d'exécution pour la programmation en tâches est d'orchestrer la réalisation des tâches en respectant leurs dépendances.
%
These runtimes also provide a load balancing over all available hardware ressources (potentially heterogeneous) while keeping data consistency.
%Ils se chargent de la répartition des tâches sur les ressources matérielles disponibles (potentiellement hétérogènes) tout en assurant la cohérence des données.
%
Some runtime also provide memory transfer between ressources, for example between main memory and a graphical card or just between two processes.
%
These transfers could be implicit, the runtime has data knowledge, or they could be explicit, a special task.


Some runtime use schedule policy (static or dynamic) to improve load balancing.
%
But the perfect scheduler doesn't exist maybe will never exist.
%
Indeed, finding the best schedule for a set of tasks on a limited number of resources machine is a NP-complete problem\footnote{A problem is NP-complete if the solving time is polynomial compare to input data.}.
%
Some heuristic are used to obtain some reasonably good results, for example the greedy algorithm.
%
Also, some heuristic needs more information about tasks, like an estimated cost.


\subsection{Timeline of runtime}
Back in 80's, some scientist succeed in interconnecting some processors together and obtain a parallel machine,

A new idea appears to make scheduler , it's name is {\em work-stealing}.
La notion de vol de tâches (en anglais \textit{work-stealing}) apparaît ensuite, ce qui rend les ordonnanceurs dynamiques.
%
With this idea, tasks can be schedule during run-time, when a hardware resource is free.
%
One of the best example is Cilk~\cite{Cilk}, first published in 1994 and still developed under the Cilk++~\cite{Cilk++}.
%
Tasks are describe by the programmer with additional keywords to the C language, for example in Cilk the keyword {\em spawn} before a function call means that a task must be create and the runtime can schedule it.
%
In the case of Cilk a specific compiler must be use in order to transform the code into a set runtime call, this is close of source-to-source compiler.
%
The parallelism that consist in spawning tasks and then wait for some of them is often called {\em insert task} paradigm.
%
One can also used the name {\em sequential task graph} because the task graph is build along the computation.
%
On the opposite, one can found {\em Parametrized Task Graph} or in a shorter way {\em PTG} where the graph .

OpenMP~\cite{OpenMP} is a different from above, it is a runtime specialized in loop parallelism without data dependency.
%
Somewhat like Cilk, it uses some language extension ({\em \#pragma omp <action>} in C) and a specific compiler must be used to translate the code (transform the loop into a function and call the right runtime functions).
%
In 1997, the specification 3.0 of OpenMP add the keyword {\em task} to support {\em sequential task graph}, it still doesn't support implicit data dependencies.
%
More recently, the specification 4.0 add an support of implicit data dependencies.
%
To date, OpenMP is certainly the most commonly used runtime through that one can obtain a parallel application with the simple well know line {\em \#pragma omp parallel for}


At begin of 2000's, architecture in the processor became parallel.
%
Two or more cores cohabit on a single chip and share some resources like cache or memory bandwidth.
%
One can have multi threading parallelism, but there is need of runtime to exploit this parallelism.
%
For example, Intel TBB (\textit{Threading Building Blocks})~\cite{TBB} which is developed by Intel since 2006 especially for abstract multi-core programming and with the same goal SMPSs~\cite{SMPSs}(now part of STARSs, see below) since 2007.

When we use a cluster, there is two choice.
%
On the one hand, there is DSM\footnote{Distributed Shared Memory}, which is very simple to use but
%
On the other hand, there is message passing paradigm, which is more difficult to use but give good result.
%
It is the most used paradigm for distributed memory especially with the MPI norm.
%

More recently, in the late 2000's, the GPGPU revolution leads the apparition of new type of runtime.
%
These runtimes must now support heterogeneous resources with sometimes several address spaces.
%
They must also integrate a support to automatically transfer data between the several address space.
%
Among these runtimes, one can found StarPU~\cite{ATNW2011,Aug2011}, PaRSEC~\cite{PaRSEC} (formerly called DAGuE~\cite{DAGuE2012}), X-KAAPI~\cite{X-KAAPI} and STARSs (became OMPSs~\cite{OMPSs} by the merge of SMPSs, ClusterSs and CellSs,\dots).

Some runtime specialized in loop parallelism also gain a GPU support, like OpenMP in is specification 4.0.
%
But this specification is mostly inspired from OpenACC~\cite{OpenACC} which wants to simplify parallel programming of heterogeneous CPU/GPU systems.

Meanwhile, Intel prepare a against GPU, the {\em Kinghts Corner}.
%
A PCI express card with around 60 cores with the well known architecture x86, more precisly an old pentium architecture with a very powerful SIMD unit.
%
The main advantage of this architecture, is that the card can be seen as a computer with 60 cores totally independent because a standard kernel can run inside.
%
So almost all runtime are directly compatible or just need minor changes.



\subsection{Classification of runtime}
We can try to classify all these runtime by functionality, we can consider three form of parallelism expression : loop parallelism, PTG and insert task paradigm.
%
The first table~\ref{tab:runtime_family} summarizes capabilities of a set of runtime, a {\it ++} entry means that the capacity is often put forward in publication, a single {\it +} means that the runtime has the functionality but it is not a major advantage of the runtime.
%
The second table~\ref{tab:runtime_archi} summarizes target architecture of the same set of runtime.
%
In the case of distributed memory, we can see two method to address this problem.
An implicit method when the runtime has a memory manager and can do automatic transfer.
Or an explicit method often describe as user task, some runtime, like StarPU, support asynchronous transfer, without this support the explicit method is less efficient.


We can also try to differentiate other differences, like the use of an API ({\it Application Programming Interface}) or the use of a source-to-source compiler.
%
Data management is also important in a runtime to optimize data transfer between CPU and GPU, or between two CPUs.

\begin{table}[h!]
\centering
\begin{tabular}{c|ccc}
  \textit{runtime}& loop parallelism & PTG & Insert task\\
  \hline
        Cilk           &    &    & ++ \\
        Cilk++         & ++ &    & ++ \\
        OpenMP $<$ 3.0 & ++ &    &    \\
        OpenMP 3.x     & ++ &    & +  \\
        OpenMP 4.0     & ++ &    & +  \\
        OpenACC        & ++ &    & +  \\
        Intel TBB      & +  &    & ++ \\
        OMPSs          & +  &    & ++ \\
        Intel CnC      & +  & ++ &    \\
        PaRSEC         &    & ++ &    \\
 PGAS(coarray fortran) & ++ &    &    \\
        StarPU         &    &    & ++ \\
        KAAPI          & ++ &    &    \\
        X-KAAPI        & +  &    & ++
\end{tabular}
\caption{Classification of runtime by capabilities}
\label{tab:runtime_family}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{c|ccc}
  \textit{runtime} & Shared memory & Distributed memory & GPU accelerator \\
\hline
        Cilk           & X &           &   \\
        Cilk++         & X &           &   \\
        OpenMP $<$ 3.0 & X &           &   \\
        OpenMP 3.x     & X &           &   \\
        OpenMP 4.0     & X &           & X \\
        OpenACC        & X &           & X \\
        Intel TBB      & X &           &   \\
        OMPSs          & X & explicit  & X \\
        Intel CnC      & X & implicit  &   \\
        PaRSEC         & X & implicit  &   \\
 PGAS(coarray fortran) & X & implicit  &   \\
        StarPU         & X & implicit/explicit  & X \\
        KAAPI          & X &           & X \\
        X-KAAPI        & X & explicit  & X
\end{tabular}
\caption{Classification of runtime by supported architecture}
\label{tab:runtime_archi}
\end{table}
