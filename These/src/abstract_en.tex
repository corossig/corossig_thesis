Solving large sparse linear system is an essential part of numerical simulations. These resolve can take up to 80\% of the total of the simulation time.
An efficient parallélization of sparse linear kernels leads to obtain better performances. In distributed memory, parallélization of theses kernels are often done by changing the numerical scheme. Contrariwise, in shared memory, a more efficient parallelism can be used. It's significant to use two levels of parallelism, a first one between nodes of a cluster and a second inside a node.
When using iterative method in shared memory, task-based programming enable the possibility to naturally describe the parallelism by using as granularity one line of the matrix for one task. Unfortunately, this granularity is too fine and doesn't allow us to obtain good performance.
In this thesis, we study the granulartiy problem of the task-based parallélization. We offer to increase grain size of computational tasks by creting aggregates of tasks which will become tasks themself. The new coarser task graph is composed by the set of these aggregates and the new dependencies between aggregates. Then a task scheduler schedule this new graph to obtain better performance. We use as example the Incomplete LU factorization of a sparse matrix an we show some improvements made by this method. Then, we focus on NUMA architecture computer. When we use a memory bandwidth limited algorithm on this architecture, it is interesting to reduce NUMA effects. We show how to take into account these effects in a task-based runtime in order to improve performance of a parallel program.

Keywords : parallelism, task-based programming, runtime, NUMA, multicore, sparse linear algebra
