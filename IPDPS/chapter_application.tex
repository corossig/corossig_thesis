% ST: TODO: expliquer ce que sont les valeurs en gras

%\begin{itemize}
%  \item Explain k in ILU(k) and impact on task size
%  \item Factorization have little data reuse
%  \item Solve don't have data reuse
%  \item Facto time, aggregation improve speed-up but less than domain decomposition
%  \item Solve time, same as facto
%  \item Agg time, need to be considerate, could be avoid by many facto/solve or compute offline if possible
%  \item We have a data locality problem and task aren't big enough
%  \item benefit of task scheduling in load balancing
%\end{itemize}

To illustrate the benefit of aggregation, we propose to study the parallelization of an iterative 
solver for sparse linear system. 
%A popular iterative method a GMRES preconditioned 
%sparse linear algebra, the GMRES with an ILU(k) preconditionner~\cite{Saad96IMSLS}.
%In this GMRES we have two major steps, the first step is the {\em ILU matrix factorization}
%and the second step is the {\em triangular solve}.
A popular approach to solve large sparse linear systems of equation is a Krylov
method (like GMRES or Conjugate Gradient) preconditioned by an incomplete
factorization (see~\cite{Saad96IMSLS}).
This is often the most time consuming part of a numerical simulation.
In this method there is two major operations, the first step is the {\em ILU matrix factorization}
and the second step is the {\em triangular solve}.
%The usual way to parallelize those kind of solver is to use a weaker form of
%the preconditioner in parallel by preconditioning subdiagonal block of the
%matrix: the subdiagonal block are usually obtained thank to a partition of
%the adjacency graph of the matrix.
Outside the preconditioner, the operations required in a Krylov method are
mostly BLAS1 operations and matrix vector products which naturally dealt with
parallel loop splitting.
In our experiments, we focus on the fine grain parallelization of
the ILU preconditioner; that is to say the factorization of the matrix (done once per system solve) and
the triangular system solves (done once at each iteration of the system solve).


Both of those algorithms (factorization and triangular solve) can naturally be represented as a task graph. For example in the factorization 
a task corresponds to the factorization of a matrix row and the dependencies are directly given by the non-zero pattern of the matrix.
%Indeed, the factorization of row $i$ can be done only when all the rows which index corresponds to an entry in the non-zero array of row $i$ before the diagonal have been factorized.
Indeed, to factorize the row $i$, we need to factorize all the rows $j$ lower than $i$ 
% which have an entry in the non-zero array of row $i$.
such that the entry $(i,j)$ is non-zero.
Therefore the DAG description is easily built from the non-zero pattern of the matrix:
the task $i$ corresponds to row $i$ of the matrix, the predecessor tasks of task $i$ are given by the column index of non-zero coefficients before the diagonal in row $i$
 and successor tasks of task $i$ are given by the row index of non-zero coefficients below the diagonal in the column $i$.

Incomplete factorization and associated triangular solve of a sparse matrix
is a problem that is well representative of the difficulty that one can encounter
with fine-grain parallelization: The fine grain description of these algorithms
is natural, but in practice, a straightforward task based parallelization using TBB or
OpenMP does not deliver good speed-ups because of the very low computational cost of
a task and the NUMA effect when accessing the coefficient of the matrix and vector.
In the experiment results, we will evaluate our programming model on those algorithms.

Our testing machine is composed of two Intel Xeon X5570 (Nehalem)
4-core sockets with 24\,GB RAM (12\,GB per socket) running Red Hat Enterprise Linux 5.2
with a Linux kernel 2.6.18.
The compiler used is Intel C++ compiler XE 13 with level 3
optimizations. The back-end task scheduling runtime is Intel TBB. We obtain comparable
results using OpenMP tasks.

We perform the tests on two linear systems. The first one, Cube\_100, is a
system obtained from a 7 points discretization scheme (e.g., finite volume) of regular 
3D cubes with 100 points along each dimension. 
It is a scalar system; each row of the matrix is a vector of coefficients stored in double precision. 
The second one, SPE10, corresponds to a 7 points discretization of a reference problem from petroleum industry~\cite{spe10}. 
In this case each entry of the matrix is a small dense block $(3,3)$ of coefficients stored in double precision.
Due to the geometry of the problem, for Cube\_100, the task dependency graph is very regular whereas it is an irregular one in the SPE10 case.
The characteristics of the matrices are listed in Table~\ref{matrices}.

\begin{table}[!h]
  \renewcommand{\arraystretch}{1.3}
  \caption{Matrices used in tests.}
  \label{matrices}
  \centering
  \begin{tabular}{|c||c|c|}
    \hline
    Name & Cube\_100 & SPE10\\
    \hline
    \hline
%    Type & Synthetic & Real Life Case\\
%    \hline
    Rows ($n$) & 1,000,000 & 3,283,263\\
    \hline
    Number of non-zeros ($nnz$) & 6,940,000 & 67,303,269\\
    \hline
    Entries of the matrix & Scalars & $(3,3)$ dense blocks\\
    \hline
  \end{tabular}
\end{table}

Each fine grain task performs one elementary line operation. SPE10
tasks weight 2-5 times more than Cube\_100 tasks. However, both
cases generate approximately 1 million tasks each.

The test protocol is the following:
\begin{itemize}
\item Each test is run three times, the final retained result is
  computed as the average of the three measured results. For each
  test, we collect three different timings:
  factorization time, triangular solve time, and aggregation time.

\item We perform the tests on a single socket (using
      {\em numactl --cpunodebind}) and on two sockets.

  % OA: je pense qu'un mot d'explication sur l'ordering sera
  % necessaire
  
\item We test two orderings (set of row/column permutations): A {\em natural} ordering which corresponds to no
      modification on matrix structure where unknowns are sequentially
      ordered by plane along the geometric z axis
      (this leads to a perfect seven diagonals pattern for Cube\_100)
      and a {\em nested dissection}~\cite{Saad96IMSLS} ordering which exposes more parallelism.

  % ST: TODO: expliquer brièvement ce que sont les natural et nest
  % dissection orderings.
  % ST: TODO: expliquer pourquoi ça n'a pas de sens d'appliquer
  % D() au cas nested dissection.

\item We test three levels of ILU(k) fill: 0, 1 and 2.
      The level $k$ of ILU(k) preconditioner determines the level of fill of
      the matrix. In others words, computation per line and the number of dependencies
      between tasks grow up with a greater $k$. Table~\ref{tab:edges} gives the number of edges resulting in the DAG of the factorization 
      (triangular solve DAG has a similar number of edges) for ILU$(0)$, ILU$(1)$ and ILU$(2)$. Subsection \ref{precond_step} will also give some features on the cost of a computational task (one row factorization) 
      depending on the ILU level of fill parameter.  

\item For the parameterized aggregation heuristics, we select the
      parameter values leading to the best performance result. With
      natural ordering, we can use the Cache Oriented algorithm because of
      particular DAG structure. With nested dissection ordering, the Cache Oriented algorithm
      can't be used, so we use the Depth Front algorithm with a high value.
\end{itemize}

In all tables, best results are represented in bold.


\begin{table}[!h]
  \renewcommand{\arraystretch}{1.3}
  \caption{Number of edges in computation DAG}
  \label{tab:edges}
  \centering
  \begin{tabular}{|c|c||c|}
    \hline
     Matrix & ILU & Edges\\
     \hline
     \hline
     CUBE\_100 & ILU(0) & 2,970,000\\
     Natural   & ILU(1) & 5,910,300\\
     ordering  & ILU(2) & 10,761,498\\
     \hline
     SPE10     & ILU(0) & 2,970,000\\
     Natural   & ILU(1) & 5,910,300\\
     ordering  & ILU(2) & 11,357,865\\
     \hline
   \end{tabular}
\end{table}
%-------------------------------

% OA: TODO: relire la section ``domain decomposition'' pour l'instant
% elle est trop courte.


%-------------------------------
% \subsection{Domain decomposition}
% With domain decomposition we have a total parallel application,
% in this way we can estimate an upper limit we can reach without
% domain decomposition.

% CR: J'enlève la decomposition de domaine car l'article est maintenant
% centré sur les problèmes NUMA

% ST: TODO: Pourquoi avoir enlevé les mesures ? sans cela on se
% demande à quoi sert la section (surtout quand on ne connait pas
% GMRES). Ou alors expliquer clairement pourquoi on ne les met pas.

%% \begin{table}[!h]
%%   \renewcommand{\arraystretch}{1.3}
%%   \caption{Result of domain decomposition with MPI in ILU(0)}
%%   \label{tab:domain}
%%   \centering
%%   \begin{tabular}{|c|c||c|c|c|c|}
%%     \hline
%%     Matrix & Method & Sequential & 4 domains & 8 domains\\
%%     &     &  (second)  & \multicolumn{2}{c|}{(speed-up)}\\
%%     \hline
%%     \hline
%%     \hline
%%     CUBE\_100   & Facto & 0.056 & 3.83 & 7.45\\
%%     No ordering & Solve & 0.923 & 2.14 & 4.23\\
%%     \hline
%%     CUBE\_100         & Facto & 0.120 & 3.71 & 7.14\\
%%     Nested dissection & Solve & 1.305 & 2.12 & 3.88\\
%%     \hline
%%     \hline
%%     SPE10       & Facto & 0.189 & 2.47 & 4.83\\
%%     no ordering & Solve & 9.884 & 1.99 & 3.82\\
%%     \hline
%%     SPE10             & Facto & 0.263 & 3.40 & 6.58\\
%%     Nested dissection & Solve & 10.24 & 1.99 & 3.82\\
%%     \hline
%%   \end{tabular}
%% \end{table}


%-------------------------------
\subsection{ILU(k) Factorization Step}
\label{precond_step}
The first test series is performed on the ILU(k) factorization step
on a single 4-core socket (Tables~\ref{tab:tbb:4:facto:no},~\ref{tab:tbb:4:facto:nested}).

% ST: TODO: mentionner que ça consiste ici en trois phases de
% préconditionnement: ILU(0), ILU(1), ILU(2), et indiquer brièvement la
% différence entre ces phases. Tout le monde ne connait pas
% GMRES, loin de là:)
With aggregation disabled the task-parallel ILU(0) factorization is always
slower than the sequential version. This is due to the additional cost
of task management. Tasks duration for CUBE\_100 in ILU(0) is only $50\,ns$ and 
$240\,ns$ for SPE10, but one task
management duration is approximately $500\,ns$. In ILU(2), tasks are bigger, their
duration is $600\,ns$ for CUBE\_100 and $1.7\,\mu{s}$ for SPE10 but it's
not bigger enough to consider task management negligible.
Another important aspect of the ILU factorization and triangular solve is that on our testbed machine 
the algorithm speed-up is bounded by the memory bandwidth: Therefore the maximum theoretical speed-up achievable 
by such algorithms on several cores is less than the number of cores used. 

With aggregation enabled and {\em CD(4)} coarse strategy string, we now reduce the number
of tasks to 2,500 with a task duration 400 times bigger.
In ILU(0) we achieve a moderate speed-up of 2. ILU(2) factorization achieves a
better speed-up of 3. The Front algorithm is not as effective as
the Depth Front in this test case, because it doesn't aggregate tasks
with continuous lines, which cause many cache misses.

% ST: TODO: Que veut dire contiguous data?
% ST: TODO: est-ce que ça aide de faire plusieurs D(4) ? En faire
% plus aurait-il aidé plus ? Discuter de quand s'arrêter ?

With the Front algorithm ({\em F(32)}) in ILU(0) with natural ordering,
%tasks at the begin and the end of the DAGs are not aggregated,
we aggregate a maximum of 157 tasks together and, on average, we aggregate only 53 tasks.

\begin{table}[!h]
  \renewcommand{\arraystretch}{1.3}
  \caption{Results on the ILU(k) factorization step on a single 4-core
    socket with TBB with natural ordering.}
  \label{tab:tbb:4:facto:no}
  \centering
  \begin{tabular}{|c|c||c|c|c|c|}
    \hline
    Matrix & ILU & Sequential & No agg. & F(32) & CD(4)\\
    &     &  (second)  & \multicolumn{3}{c|}{(speed-up)}\\
    \hline
    \hline
    \hline
    & ILU(0) & 0.056 & 0.20 & 1.06 & 2.23\\
    CUBE\_100 & ILU(1) & 0.142 & 0.46 & 1.54 & 2.81\\
    & ILU(2) & 0.611 & 1.30 & 2.58 & {\bf 3.47}\\
    \hline
    & ILU(0) & 0.262 & 0.65 & 1.89 & 3.09\\
    SPE10     & ILU(1) & 0.721 & 1.39 & 2.30 & 3.22\\
    & ILU(2) & 1.936 & 1.87 & 2.19 & 3.32\\
    \hline
  \end{tabular}
\end{table}


\begin{table}[!h]
  \renewcommand{\arraystretch}{1.3}
  \caption{Results on the ILU(k) factorization step on a single 4-core
    socket with TBB with nested dissection ordering.}
  \label{tab:tbb:4:facto:nested}
  \centering
  \begin{tabular}{|c|c||c|c|c|c|}
    \hline
    Matrix & ILU & Sequential & No agg. & F(32) & D(400)\\
    &     &  (second)  & \multicolumn{3}{c|}{(speed-up)}\\
    \hline
    \hline
    & ILU(0) & 0.129 & 0.46 & 2.17 & 2.26\\
    CUBE\_100 & ILU(1) & 0.495 & 1.29 & 1.70 & 2.78\\
    & ILU(2) & 0.828 & 1.74 & 1.94 & {\bf 3.12}\\
    \hline
    & ILU(0) & 0.276 & 0.74 & 1.93 & 2.16\\
    SPE10     & ILU(1) & 1.375 & 1.98 & 1.67 & 2.82\\
    & ILU(2) & 2.247 & 2.43 & 1.85 & {\bf 3.12}\\
    \hline
  \end{tabular}
\end{table}

With two 4-core sockets (Tables~\ref{tab:tbb:8:facto:no},~\ref{tab:tbb:8:facto:nested}), the parallel
ILU(0) again performs slower than sequential execution when
aggregation is disabled. With aggregation enabled, the ILU(0)
achieves a speed-up of 3. ILU(2) achieves a speed-up of 6.2.

\begin{table}[!h]
  \renewcommand{\arraystretch}{1.3}
  \caption{Results on the ILU(k) factorization step on two 4-core
    sockets with TBB with natural ordering.}
  \label{tab:tbb:8:facto:no}
  \centering
  \begin{tabular}{|c|c||c|c|c|c|}
    \hline
    Matrix & ILU & Sequential & No agg. & F(32) & CD(4)\\
    &     &  (second)  & \multicolumn{3}{c|}{(speed-up)}\\
    \hline
    \hline
    \hline
    & ILU(0) & 0.056 & 0.28 & 1.27 & 2.54\\
    CUBE\_100 & ILU(1) & 0.143 & 0.65 & 1.89 & 3.79\\
    & ILU(2) & 0.612 & 1.47 & 3.06 & 3.91\\
    \hline
    & ILU(0) & 0.260 & 0.97 & 2.48 & 3.78\\
    SPE10     & ILU(1) & 0.771 & 2.24 & 3.80 & 5.72\\
    & ILU(2) & 2.006 & 3.37 & 3.97 & {\bf 6.21}\\
    \hline
  \end{tabular}
\end{table}


\begin{table}[!h]
  \renewcommand{\arraystretch}{1.3}
  \caption{Results on the ILU(k) factorization step on two 4-core
    sockets with TBB with nested dissection ordering.}
  \label{tab:tbb:8:facto:nested}
  \centering
  \begin{tabular}{|c|c||c|c|c|c|}
    \hline
    Matrix & ILU & Sequential & No agg. & F(32) & D(400)\\
    &     &  (second)  & \multicolumn{3}{c|}{(speed-up)}\\
    \hline
    \hline
    & ILU(0) & 0.127 & 0.41 & 3.10 & 3.31\\
    CUBE\_100 & ILU(1) & 0.483 & 1.31 & 2.70 & 4.77\\
    & ILU(2) & 0.817 & 1.96 & 3.18 & 5.46\\
    \hline
    & ILU(0) & 0.277 & 0.71 & 2.59 & 3.09\\
    SPE10     & ILU(1) & 1.452 & 2.48 & 2.81 & 5.00\\
    & ILU(2) & 2.347 & 3.29 & 3.12 & {\bf 5.57}\\
    \hline
  \end{tabular}
\end{table}

%-------------------------------
\subsection{Triangular Solve Step}\label{subsec:solve}

The triangular solve step is itself composed of two parts: A
{\em forward} substitution followed by a {\em backward}
substitution. The DAG of the forward substitution is identical to
the DAG of the factorization step mentioned in the previous section.
Thus we can reuse the same coarse
DAG. In our test cases, the DAG of the backward substitution part is
the transpose of the DAG of the forward substitution. Here again,
the factorization coarse DAG can thus straightforwardly be reused.
In total we have twice more tasks in triangular solve than in factorization.

The weight of operations done in triangular solve elementary tasks
is lighter than their factorization task counterparts.
%% and these tasks need to access to more data..
Tests on a
single 4-core socket (Tables~\ref{tab:tbb:4:solve:no},~\ref{tab:tbb:4:solve:nested})
show that the parallel triangular solve is always slower than
the sequential version with task aggregation disabled. With
aggregation enabled, we obtain a speedup of~2.

% ST: TODO: idéalement il faudrait que ces deux tables se retrouvent
% côte à côte dans la version finale.
\begin{table}[!h]
  \renewcommand{\arraystretch}{1.3}
  \caption{Results on the Triangular Solve step on a single 4-core
    socket with TBB with natural ordering.}
  \label{tab:tbb:4:solve:no}
  \centering
  \begin{tabular}{|c|c||c|c|c|c|}
    \hline
    Matrix & ILU & Sequential & No agg. & F(32) & CD(4)\\
    &     &  (second)  & \multicolumn{3}{c|}{(speed-up)}\\
    \hline
    \hline
    & ILU(0) & 0.092 & 0.23 & 0.88 & 1.90\\
    CUBE\_100 & ILU(1) & 0.117 & 0.27 & 0.82 & 1.97\\
    & ILU(2) & 0.163 & 0.34 & 0.92 & 2.05\\
    \hline
    & ILU(0) & 0.219 & 0.40 & 1.08 & 1.93\\
    SPE10     & ILU(1) & 0.353 & 0.62 & 1.37 & 2.22\\
    & ILU(2) & 0.554 & 0.85 & 1.58 & {\bf 2.39}\\
    \hline
  \end{tabular}
\end{table}

\begin{table}[!h]
  \renewcommand{\arraystretch}{1.3}
  \caption{Results of the Triangular Solve step on a single 4-core
    socket with TBB with nested dissection ordering.}
  \label{tab:tbb:4:solve:nested}
  \centering
  \begin{tabular}{|c|c||c|c|c|c|}
    \hline
    Matrix & ILU & Sequential & No agg. & F(32) & D(400)\\
    &     &  (second)  & \multicolumn{3}{c|}{(speed-up)}\\
    \hline
    \hline
    & ILU(0) & 0.107 & 0.20 & 1.19 & 1.34\\
    CUBE\_100 & ILU(1) & 0.150 & 0.31 & 0.94 & 1.39\\
    & ILU(2) & 0.171 & 0.34 & 0.95 & 1.50\\
    \hline
    & ILU(0) & 0.249 & 0.37 & 1.39 & 1.52\\
    SPE10     & ILU(1) & 0.430 & 0.65 & 1.32 & 1.77\\
    & ILU(2) & 0.500 & 0.72 & 1.35 & {\bf 1.89}\\
    \hline
  \end{tabular}
\end{table}

On two 4-core sockets
(Tables~\ref{tab:tbb:8:solve:no},~\ref{tab:tbb:8:solve:nested}), with
task aggregation disabled, only the SPE10 test achieves a speedup
greater than 1. With aggregation enabled, a speedup of 2.52 is
achieved on ILU(0) and a speedup of 4.14 is achieved on ILU(2).

\begin{table}[!h]
  \renewcommand{\arraystretch}{1.3}
  \caption{Results of the Triangular Solve step on two 4-core sockets with
    TBB with natural ordering.}
  \label{tab:tbb:8:solve:no}
  \centering
  \begin{tabular}{|c|c||c|c|c|c|}
    \hline
    Matrix & ILU & Sequential & No agg. & F(32) & CD(4)\\
    &     &  (second)  & \multicolumn{3}{c|}{(speed-up)}\\
    \hline
    \hline
    & ILU(0) & 0.092 & 0.27 & 1.27 & 2.44\\
    CUBE\_100 & ILU(1) & 0.123 & 0.35 & 1.54 & 2.82\\
    & ILU(2) & 0.174 & 0.45 & 1.40 & 2.98\\
    \hline
    & ILU(0) & 0.219 & 0.53 & 1.63 & {\bf 2.52}\\
    SPE10     & ILU(1) & 0.408 & 0.96 & 2.39 & 3.77\\
    & ILU(2) & 0.658 & 1.38 & 2.79 & {\bf 4.14}\\
    \hline
  \end{tabular}
\end{table}


\begin{table}[!h]
  \renewcommand{\arraystretch}{1.3}
  \caption{Results of the Triangular Solve step on two 4-core
    sockets with TBB with nested dissection ordering.}
  \label{tab:tbb:8:solve:nested}
  \centering
  \begin{tabular}{|c|c||c|c|c|c|}
    \hline
    Matrix & ILU & Sequential & No agg. & F(32) & D(400)\\
    &     &  (second)  & \multicolumn{3}{c|}{(speed-up)}\\
    \hline
    \hline
    & ILU(0) & 0.107 & 0.18 & 1.41 & 1.67\\
    CUBE\_100 & ILU(1) & 0.156 & 0.32 & 1.36 & 1.92\\
    & ILU(2) & 0.180 & 0.36 & 1.37 & 2.08\\
    \hline
    & ILU(0) & 0.249 & 0.35 & 1.64 & 1.82\\
    SPE10     & ILU(1) & 0.496 & 0.80 & 2.12 & 2.71\\
    & ILU(2) & 0.578 & 0.90 & 2.19 & {\bf 2.90}\\
    \hline
  \end{tabular}
\end{table}

% In most cases, domain decomposition achieves better raw performance
% results. However, by avoiding potentially harmful domain boundaries artifacts, the
% aggregation method ensures a better overall numerical stability than the
% domain decomposition.

%-------------------------------
\subsection{Aggregation Overhead}
This section evaluates the overhead caused by the task aggregation
step for several aggregation algorithm instances. As mentioned in
the test description above, the fine-grain DAG generated from the
test cases amount to about 1 million nodes for each matrix. The 
number of edges increases when the parameter $k$ of ILU(k) preconditioner increases (see Table~\ref{tab:edges}).
This aggregation step is performed only once for each matrix. Then, the
coarsened DAG is reused as long as the sparse pattern of the matrix is unchanged.
In classical numerical simulation, the mesh on which the equations are discretized
does not change through the simulation, only the coefficients of the linear system
are changing: This means that the aggregation step is typically done only once
while the factorization and triangular solves are called a large number of times (several thousand of times).
The time spent in the task aggregation (Table~\ref{agg_time}) is then usually negligible
compared to the time spent in the factorizations and triangular solves.


On average, applying {\em CD(4)} Coarse String takes 1.5\,s on a DAG with 1 million
of nodes and applying {\em D(400)} takes 3.5\,s.

For {\em CD(4)} aggregation we won on average 0.22\,s per factorization
and 0.31\,s per triangular solve compared to not doing aggregation. So
after only 3 combinations of factorization and triangular solve, the aggregation
become profitable.

For {\em D(400)} aggregation we won in average 0.28\,s per factorization
and 0.49\,s per triangular solve. So after only 5 combinations of factorization
and triangular solve, the aggregation becomes profitable.
% ST: TODO: et donc comparer ce coût par rapport à n fois le gain
% obtenus à chaque phase.

\begin{table}[!h]
  \renewcommand{\arraystretch}{1.3}
  \caption{Aggregation overhead.}
  \label{agg_time}
  \centering
  \begin{tabular}{|c|c||c|c|c|c|}
    \hline
    Matrix & ILU & F(32) & D(400) & CD(4)\\
    &     &  \multicolumn{3}{c|}{(second)}\\
    \hline
    \hline
    \hline
    CUBE\_100  & ILU(0) & 1.534 & 2.945 & 0.908\\
    No         & ILU(1) & 1.467 & 3.032 & 1.119\\
    ordering   & ILU(2) & 1.871 & 3.705 & 1.315\\
    \hline
    CUBE\_100  & ILU(0) & 1.208 & 2.330 & N/A\\
    Nested     & ILU(1) & 2.194 & 3.224 & N/A\\
    dissection & ILU(2) & 3.182 & 3.554 & N/A\\
    \hline
    \hline
    SPE10      & ILU(0) & 1.519 & 3.239 & 1.217\\
    no         & ILU(1) & 1.542 & 3.344 & 1.527\\
    ordering   & ILU(2) & 2.019 & 4.042 & 1.918\\
    \hline
    SPE10      & ILU(0) & 1.292 & 2.541 & N/A\\
    Nested     & ILU(1) & 2.337 & 3.518 & N/A\\
    dissection & ILU(2) & 3.119 & 3.816 & N/A\\
    \hline
  \end{tabular}
\end{table}
